{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n['D:\\\\Users\\\\MC\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\api\\\\_v1', 'D:\\\\Users\\\\MC\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow_estimator\\\\python\\\\estimator\\\\api\\\\_v1', 'D:\\\\Users\\\\MC\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow', 'D:\\\\Users\\\\MC\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1']\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# ## 1）环境准备\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Builds a set of wide and deep feature columns.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## 2）数据准备\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "# 定义输入样本格式\n",
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "\"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_columns():\n",
    "    # 1. 特征处理，包括：连续特征、离散特征、转换特征、交叉特征等\n",
    "\n",
    "    # Base Continuous Feature Columns,连续特征 （其中在Wide和Deep组件都会用到）\n",
    "    age = tf.feature_column.numeric_column('age')\n",
    "    education_num = tf.feature_column.numeric_column('education_num')\n",
    "    capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "    capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "    hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "    # 离散特征\n",
    "    education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'education', [\n",
    "            'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "            'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "            '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "    marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'marital_status', [\n",
    "            'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "            'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "    # Base Categorical Feature Columns\n",
    "    # 如果我们知道所有的取值，并且取值不是很多\n",
    "    relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'relationship', [\n",
    "            'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "            'Other-relative'])\n",
    "\n",
    "    workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'workclass', [\n",
    "            'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "            'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "    # # 如果不知道有多少取值,离散hash bucket特征\n",
    "    occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'occupation', hash_bucket_size=1000\n",
    "    )\n",
    "\n",
    "    #     Sometimes the relationship between a continuous feature and the label is not linear. As a hypothetical example, a person's income may grow with age in the early stage of one's career, then the growth may slow at some point, and finally the income decreases after retirement. In this scenario, using the raw age as a real-valued feature column might not be a good choice because the model can only learn one of the three cases:\n",
    "\n",
    "    # 3.2.1 连续特征离散化\n",
    "    # 之所以这么做是因为：有些时候连续特征和label之间不是线性的关系。可能刚开始是正的线性关系，后面又变成了负的线性      #关系，这样一个折线的关系整体来看就不再是线性关系。\n",
    "    # bucketization 装桶\n",
    "    # 10个边界，11个桶\n",
    "    # 特征Transformations\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "        age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    "    )\n",
    "\n",
    "    # 2. 设定Wide层特征,The Wide Model: Linear Model with CrossedFeatureColumns\n",
    "    \"\"\"\n",
    "    The wide model is a linear model with a wide set of *sparse and crossed feature* columns\n",
    "\tWide部分用了一个规范化后的连续特征age_buckets，其他的连续特征没有使用;\n",
    "    Wide部分使用了规范化后的连续特征、离散特征、交叉特征\n",
    "    \"\"\"\n",
    "    # 基本特征列\n",
    "    \"\"\"\n",
    "    之前的特征：\n",
    "    1. CategoricalColumn\n",
    "    2. NumericalColumn\n",
    "    3. BucketizedColumn\n",
    "    4. CrossedColumn\n",
    "    这些特征都是FeatureColumn的子类，可以放到一起\n",
    "    \"\"\"\n",
    "    base_columns = [\n",
    "        # 全是离散特征\n",
    "        education, marital_status, relationship, workclass, occupation,\n",
    "        age_buckets,\n",
    "    ]\n",
    "\n",
    "    # 组合特征/交叉特征列\n",
    "    crossed_columns = [\n",
    "        tf.feature_column.crossed_column(\n",
    "            ['education', 'occupation'], hash_bucket_size=1000),\n",
    "        tf.feature_column.crossed_column(\n",
    "            [age_buckets, 'education', 'occupation'], hash_bucket_size=1000\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # wide特征列\n",
    "    wide_columns = base_columns + crossed_columns\n",
    "\n",
    "    # 3. 设定Deep层特征\n",
    "    \"\"\"\n",
    "    Deep层主要针对离散特征进行处理，其中处理方式有：\n",
    "    1. Sparse Features -> Embedding vector -> 串联(连续特征)，其中Embedding Values随机初始化。 -> 输入到Hidden Layer\n",
    "    2. 另外一种处理离散特征的方法是：one-hot和multi-hot representation. 此方法适用于低维度特征，其中embedding是更加通用的做法\n",
    "    其中：采用embedding_column(embedding)和indicator_column(multi-hot)API\n",
    "    \"\"\"\n",
    "    # deep特征列\n",
    "    deep_columns = [\n",
    "        age,\n",
    "        education_num,\n",
    "        capital_gain,\n",
    "        capital_loss,\n",
    "        hours_per_week,\n",
    "        tf.feature_column.indicator_column(workclass),\n",
    "        tf.feature_column.indicator_column(education),\n",
    "        tf.feature_column.indicator_column(marital_status),\n",
    "        tf.feature_column.indicator_column(relationship),\n",
    "\n",
    "        # embedding特征:To show an example of embedding\n",
    "        tf.feature_column.embedding_column(occupation, dimension=8)\n",
    "    ]\n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator Input\n",
    "# 定义输入\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"为Estimator创建一个input function\"\"\"\n",
    "    assert tf.gfile.Exists(data_file), \"{0} not found.\".format(data_file)\n",
    "\n",
    "    def parse_csv(line):\n",
    "        print(\"Parsing\", data_file)\n",
    "        # tf.decode_csv会把csv文件转换成Tensor,一列一个。其中record_defaults用于指明每一列的缺失值用什么填充。\n",
    "        columns = tf.decode_csv(line, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop('income_bracket')\n",
    "        # tf.equal(x, y) 返回一个bool类型Tensor， 表示x == y, element-wise\n",
    "        return features, tf.equal(labels, '>50K')\n",
    "\n",
    "    dataset = tf.data.TextLineDataset(data_file).map(parse_csv, num_parallel_calls=5)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3）模型准备\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# Wide & Deep Model\n",
    "def build_estimator(model_dir, model_type):\n",
    "    \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "    wide_columns, deep_columns = build_model_columns()\n",
    "    hidden_units = [100, 50]\n",
    "\n",
    "    # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
    "    # trains faster than GPU for this model.\n",
    "    run_config = tf.estimator.RunConfig().replace(\n",
    "        session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "    if model_type == 'wide':\n",
    "        return tf.estimator.LinearClassifier(\n",
    "            model_dir=model_dir,\n",
    "            feature_columns=wide_columns,\n",
    "            config=run_config)\n",
    "    elif model_type == 'deep':\n",
    "        return tf.estimator.DNNClassifier(\n",
    "            model_dir=model_dir,\n",
    "            feature_columns=deep_columns,\n",
    "            hidden_units=hidden_units,\n",
    "            config=run_config)\n",
    "    else:\n",
    "        return tf.estimator.DNNLinearCombinedClassifier(\n",
    "            model_dir=model_dir,\n",
    "            linear_feature_columns=wide_columns,\n",
    "            dnn_feature_columns=deep_columns,\n",
    "            dnn_hidden_units=hidden_units,\n",
    "            config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/data/model/wide_deep', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n  key: \"GPU\"\n  value: 0\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FC14A213C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "./adult.data not found.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dfb7d16aa5d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# 模型训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m# 模型评估\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\MC\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\MC\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1156\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\MC\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1183\u001b[0m       features, labels, input_hooks = (\n\u001b[0;32m   1184\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[1;32m-> 1185\u001b[1;33m               input_fn, ModeKeys.TRAIN))\n\u001b[0m\u001b[0;32m   1186\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[1;32mD:\\Users\\MC\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[1;34m(self, input_fn, mode)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[1;32m-> 1022\u001b[1;33m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[0;32m   1023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\MC\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[1;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[0;32m   1111\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_context'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/cpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-dfb7d16aa5d5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# 模型训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m# 模型评估\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-20a2bbbd909a>\u001b[0m in \u001b[0;36minput_fn\u001b[1;34m(data_file, num_epochs, shuffle, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"\"\"为Estimator创建一个input function\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"{0} not found.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: ./adult.data not found."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 模型路径\n",
    "model_type = 'widedeep'\n",
    "model_dir = '/data/model/wide_deep'\n",
    "\n",
    "# Wide & Deep 联合模型\n",
    "model = build_estimator(model_dir, model_type)\n",
    "\n",
    "# ## 4）模型训练\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "# 训练参数\n",
    "train_epochs = 10\n",
    "batch_size = 5000\n",
    "train_file = './adult.data'\n",
    "test_file = './adult.test'\n",
    "\n",
    "# 6. 开始训练\n",
    "for n in range(train_epochs):\n",
    "    # 模型训练\n",
    "    model.train(input_fn=lambda: input_fn(train_file, train_epochs, True, batch_size))\n",
    "    # 模型评估\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(test_file, 1, False, batch_size))\n",
    "    # 打印评估结果\n",
    "    print(\"Results at epoch {0}\".format((n + 1) * train_epochs))\n",
    "    print('-' * 30)\n",
    "    for key in sorted(results):\n",
    "        print(\"{0:20}: {1:.4f}\".format(key, results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
